# -*- coding: utf-8 -*-
"""Copy of WorkoutNarok.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PHJ0CKFQA0LJIfDPMX7i1-r5aP-Vx8yG
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth "xformers==0.0.28.post2"
# # Also get the latest nightly Unsloth!
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

# !pip install triton
from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/mistral-7b-instruct-v0.2-bnb-4bit",
    "unsloth/llama-2-7b-bnb-4bit",
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit", # New Google 6 trillion tokens model 2.5x faster!
    "unsloth/gemma-2b-bnb-4bit",
] # More models at https://huggingface.co/unsloth

# model, tokenizer = FastLanguageModel.from_pretrained(
#     model_name =  "unsloth/llama-2-7b-bnb-4bit", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
#     max_seq_length = max_seq_length,
#     dtype = dtype,
#     load_in_4bit = load_in_4bit,
#     # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
# )

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "rithesh10/workoutLlama2", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)


# print("Model:", model)  # Should print the model details
# print("Tokenizer:", tokenizer)  # Should print the tokenizer details

print(model)
print(tokenizer)

# alpaca_prompt = Copied from above
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "Create a 7-day workout plan for a 30-year-old male who is a beginner at the gym. The person is 190 cm tall, weighs 90 kg, and wants to lose weight to reach 80 kg. Please vary the daily exercises, focusing on different muscle groups like chest, back, legs, shoulders, and arms, with cardio incorporated.", # instruction
        "", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 250)

# # Set up your prompt
# alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

# ### Instruction:
# {}

# ### Input:
# {}

# ### Response:
# {}"""

# # Define the instruction
# instruction = "I am a 48-year-old female with a height of 169 cm and a weight of 71 kg. My primary goal is muscle gain, and I aim to reach a target weight of 76 kg. I am a beginner at the gym. Can you create a 7-day workout plan for me?"

# # Tokenize input
# inputs = tokenizer(
#     [alpaca_prompt.format(instruction, "", "")],
#     return_tensors='pt',
#     max_length=512,  # Specify a maximum length if necessary
#     padding='max_length',  # Pad to the maximum length
#     truncation=True  # Truncate to the maximum length
# )

# # Move inputs to GPU if available

# # Initialize the TextStreamer
# text_streamer = TextStreamer(tokenizer)

# # Generate response with streaming
# _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=250)

if model is None:
    print("Model loading failed.")
else:
    print("Model loaded successfully:", type(model))

!pip install fastapi uvicorn pyngrok nest-asyncio

!ngrok authtoken 2bLpXKN8yDr3RbAHoUwGzRODoM1_58WWak7itZ54sXhE5uHWZ

import torch
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from pyngrok import ngrok
import uvicorn
import nest_asyncio
from transformers import TextStreamer
from contextlib import asynccontextmanager

# Define the request model
class QueryModel(BaseModel):
    query: str
    input_text: str = ""

# Use your pre-initialized model and tokenizer
# These should be already defined in your previous cell
external_model = model  # Your pre-initialized model
external_tokenizer = tokenizer  # Your pre-initialized tokenizer

# Lifespan context manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Use the external model and tokenizer
    global model, tokenizer
    model = external_model
    tokenizer = external_tokenizer
    yield
    # Clean up resources on shutdown
    model = None
    tokenizer = None

# Initialize FastAPI app with lifespan
app = FastAPI(lifespan=lifespan)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Alpaca prompt template
ALPACA_PROMPT = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

@app.post("/generate")
async def generate_text(request: QueryModel):
    try:
        formatted_prompt = ALPACA_PROMPT.format(
            request.query,
            request.input_text,
            ""
        )

        inputs = tokenizer(
            [formatted_prompt],
            return_tensors="pt"
        ).to("cuda")

        text_streamer = TextStreamer(tokenizer)

        output = model.generate(
            **inputs,
            streamer=text_streamer,
            max_new_tokens=250
        )

        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

        return {
            "status": "success",
            "generated_text": generated_text
        }

    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

# Run the server
if __name__ == "__main__":
    # Apply nest_asyncio for Colab compatibility
    nest_asyncio.apply()

    # Start ngrok tunnel
    public_url = ngrok.connect(8000)
    print(f"Public URL: {public_url}")

    # Start FastAPI server
    uvicorn.run(app, host="0.0.0.0", port=8000)